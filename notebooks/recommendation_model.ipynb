{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "## Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\nimport mlflow\nimport mlflow.lightgbm\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load",
   "metadata": {},
   "outputs": [],
   "source": "df_features = pd.read_csv('../data/customer_features.csv')\ndf_offers = pd.read_csv('../data/offer_interactions.csv')\n\nprint(f\"Customers: {len(df_features):,} | Offers: {len(df_offers):,} | Redemption rate: {df_offers['redeemed'].mean()*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "cluster_header",
   "metadata": {},
   "source": [
    "---\n",
    "### Stage 1: Customer Clustering (Retrieval)\n",
    "\n",
    "Group similar customers into segments using KMeans.\n",
    "Each cluster gets a set of top-performing offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster_features",
   "metadata": {},
   "outputs": [],
   "source": "cluster_feature_names = [\n    'recency_days', 'frequency', 'monetary_total', 'monetary_avg',\n    'unique_service_count', 'unique_category_count', 'unique_location_count',\n    'customer_tenure_days', 'avg_days_between_purchases'\n]\ncluster_feature_names = [c for c in cluster_feature_names if c in df_features.columns]\n\nX_cluster = df_features[cluster_feature_names].fillna(0)\n\nscaler = StandardScaler()\nX_cluster_scaled = scaler.fit_transform(X_cluster)"
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_header",
   "metadata": {},
   "source": [
    "### 3. Run KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans",
   "metadata": {},
   "outputs": [],
   "source": "N_CLUSTERS = 10\n\nkmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\ndf_features['cluster'] = kmeans.fit_predict(X_cluster_scaled)\n\ndf_features['cluster'].value_counts().sort_index()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cluster_viz",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Cluster sizes\ndf_features['cluster'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')\naxes[0].set_title('Customers per Cluster')\naxes[0].set_xlabel('Cluster')\naxes[0].set_ylabel('Count')\n\n# Heatmap of cluster profiles\ncluster_means = df_features.groupby('cluster')[cluster_feature_names].mean()\ncluster_normalized = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n\nim = axes[1].imshow(cluster_normalized.T, cmap='YlOrRd', aspect='auto')\naxes[1].set_xticks(range(N_CLUSTERS))\naxes[1].set_yticks(range(len(cluster_feature_names)))\naxes[1].set_yticklabels(cluster_feature_names, fontsize=8)\naxes[1].set_xlabel('Cluster')\naxes[1].set_title('Cluster Profiles (Normalized)')\nplt.colorbar(im, ax=axes[1])\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "affinity_header",
   "metadata": {},
   "source": [
    "### 4. Cluster \u2192 Offer Affinity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affinity",
   "metadata": {},
   "outputs": [],
   "source": "df_offers_with_cluster = df_offers.merge(\n    df_features[['customer_id', 'cluster']],\n    on='customer_id',\n    how='inner'\n)\n\ncluster_offer_affinity = df_offers_with_cluster.pivot_table(\n    values='redeemed',\n    index='cluster',\n    columns='offer_type',\n    aggfunc='mean'\n).round(3)\n\ncluster_offer_affinity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affinity_viz",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(10, 6))\n\nim = ax.imshow(cluster_offer_affinity.values, cmap='YlGn', aspect='auto')\nax.set_xticks(range(len(cluster_offer_affinity.columns)))\nax.set_xticklabels(cluster_offer_affinity.columns, rotation=45, ha='right', fontsize=9)\nax.set_yticks(range(N_CLUSTERS))\nax.set_yticklabels([f'Cluster {i}' for i in range(N_CLUSTERS)])\nax.set_title('Cluster \u00d7 Offer Redemption Rate (Stage 1 Retrieval Matrix)')\nplt.colorbar(im, ax=ax, label='Redemption Rate')\n\nfor i in range(cluster_offer_affinity.shape[0]):\n for j in range(cluster_offer_affinity.shape[1]):\n val = cluster_offer_affinity.values[i, j]\n ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=8,\n color='white' if val > 0.4 else 'black')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval_lookup",
   "metadata": {},
   "outputs": [],
   "source": "cluster_offer_id_affinity = df_offers_with_cluster.pivot_table(\n    values='redeemed',\n    index='cluster',\n    columns='offer_id',\n    aggfunc='mean'\n).round(3)\n\nRETRIEVAL_TOP_N = 5\n\ncluster_candidates = {}\nfor cluster_id in range(N_CLUSTERS):\n    top_offers = cluster_offer_id_affinity.loc[cluster_id].nlargest(RETRIEVAL_TOP_N).index.tolist()\n    cluster_candidates[cluster_id] = top_offers\n\nprint(f\"Stage 1: {RETRIEVAL_TOP_N} candidates per cluster\")"
  },
  {
   "cell_type": "markdown",
   "id": "stage2_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Stage 2: LightGBM Ranking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge_section",
   "metadata": {},
   "source": [
    "### 5. Merge & Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge",
   "metadata": {},
   "outputs": [],
   "source": "df_train = df_offers.merge(df_features, on='customer_id', how='inner')\ndf_train = pd.get_dummies(df_train, columns=['offer_type'], prefix='offer')\n\noffer_cols = [c for c in df_train.columns if c.startswith('offer_')]"
  },
  {
   "cell_type": "markdown",
   "id": "interaction_section",
   "metadata": {},
   "source": [
    "### 6. Creating Interaction Features\n",
    "\n",
    "These capture the **customer \u00d7 offer** combinations that drive redemption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactions",
   "metadata": {},
   "outputs": [],
   "source": "offer_type_cols = [c for c in df_train.columns if c.startswith('offer_') and c != 'offer_value' and c != 'offer_id' and c != 'offer_name']\n\nfor col in offer_type_cols:\n    df_train[f'freq_x_{col}'] = df_train['frequency'] * df_train[col]\n    df_train[f'recency_x_{col}'] = df_train['recency_days'] * df_train[col]\n    df_train[f'monetary_x_{col}'] = df_train['monetary_avg'] * df_train[col]\n\nif 'is_frequent' in df_train.columns:\n    for col in offer_type_cols:\n        df_train[f'isfreq_x_{col}'] = df_train['is_frequent'] * df_train[col]\n\nif 'is_recent' in df_train.columns:\n    for col in offer_type_cols:\n        df_train[f'isrecent_x_{col}'] = df_train['is_recent'] * df_train[col]\n\nif 'is_lapsed' in df_train.columns:\n    for col in offer_type_cols:\n        df_train[f'islapsed_x_{col}'] = df_train['is_lapsed'] * df_train[col]\n\nif 'is_budget' in df_train.columns:\n    for col in offer_type_cols:\n        df_train[f'isbudget_x_{col}'] = df_train['is_budget'] * df_train[col]\n\nif 'rfm_score' in df_train.columns:\n    for col in offer_type_cols:\n        df_train[f'rfm_x_{col}'] = df_train['rfm_score'] * df_train[col]\n\ninteraction_cols = [c for c in df_train.columns if '_x_' in c]"
  },
  {
   "cell_type": "markdown",
   "id": "feature_selection",
   "metadata": {},
   "source": [
    "### 7. Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select_features",
   "metadata": {},
   "outputs": [],
   "source": "leaky_features = ['open_rate', 'click_rate', 'redemption_rate', \n                  'total_opens', 'total_clicks', 'total_redemptions',\n                  'opened', 'clicked']\n\nexclude_cols = ['customer_id', 'interaction_id', 'offer_id', 'offer_name', \n                'sent_date', 'redeemed', 'favorite_category', 'favorite_offer_type']\n\nfeature_cols = [c for c in df_train.columns \n                if c not in exclude_cols \n                and c not in leaky_features\n                and df_train[c].dtype in ['int64', 'float64', 'int32', 'float32', 'uint8']]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_xy",
   "metadata": {},
   "outputs": [],
   "source": "X = df_train[feature_cols].fillna(0)\ny = df_train['redeemed']"
  },
  {
   "cell_type": "markdown",
   "id": "split_section",
   "metadata": {},
   "source": [
    "### 8. Splitting Data for the model (Train/Test Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split",
   "metadata": {},
   "outputs": [],
   "source": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)"
  },
  {
   "cell_type": "markdown",
   "id": "mlflow_header",
   "metadata": {},
   "source": [
    "### 9. Train LightGBM Model with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow_train",
   "metadata": {},
   "outputs": [],
   "source": "mlflow.set_experiment(\"Offer_Recommendation_Ranking\")\n\nwith mlflow.start_run(run_name=\"lgbm_v2_interaction_features\"):\n\n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'boosting_type': 'gbdt',\n        'num_leaves': 63,\n        'max_depth': 8,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'min_child_samples': 100,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'verbose': -1,\n        'is_unbalance': True,\n        'random_state': 42\n    }\n\n    mlflow.log_params(params)\n    mlflow.log_param('num_features', len(feature_cols))\n    mlflow.log_param('num_interaction_features', len(interaction_cols))\n    mlflow.log_param('train_samples', len(X_train))\n    mlflow.log_param('test_samples', len(X_test))\n    mlflow.log_param('positive_rate', round(y.mean(), 3))\n    mlflow.log_param('n_clusters', N_CLUSTERS)\n    mlflow.log_param('retrieval_top_n', RETRIEVAL_TOP_N)\n\n    train_data = lgb.Dataset(X_train, label=y_train, feature_name=feature_cols, free_raw_data=True)\n    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data, free_raw_data=True)\n\n    model = lgb.train(\n        params,\n        train_data,\n        num_boost_round=1000,\n        valid_sets=[test_data],\n        valid_names=['test'],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50),\n            lgb.log_evaluation(period=0)\n        ]\n    )\n\n    mlflow.log_param('best_iteration', model.best_iteration)\n\n    y_pred_proba = model.predict(X_test)\n    y_train_pred = model.predict(X_train)\n\n    test_auc = roc_auc_score(y_test, y_pred_proba)\n    train_auc = roc_auc_score(y_train, y_train_pred)\n\n    mlflow.log_metric('test_auc', round(test_auc, 4))\n    mlflow.log_metric('train_auc', round(train_auc, 4))\n    mlflow.log_metric('auc_gap', round(train_auc - test_auc, 4))\n\n    for threshold in [0.5, 0.4, 0.3, 0.2]:\n        y_pred = (y_pred_proba >= threshold).astype(int)\n        prec = precision_score(y_test, y_pred, zero_division=0)\n        rec = recall_score(y_test, y_pred, zero_division=0)\n        mlflow.log_metric(f'precision_at_{threshold}', round(prec, 4))\n        mlflow.log_metric(f'recall_at_{threshold}', round(rec, 4))\n\n    mlflow.log_metric('pred_min', round(float(y_pred_proba.min()), 4))\n    mlflow.log_metric('pred_max', round(float(y_pred_proba.max()), 4))\n    mlflow.log_metric('pred_mean', round(float(y_pred_proba.mean()), 4))\n    mlflow.log_metric('pred_std', round(float(y_pred_proba.std()), 4))\n\n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n    fig_roc, ax_roc = plt.subplots(figsize=(8, 6))\n    ax_roc.plot(fpr, tpr, 'b-', linewidth=2, label=f'Model (AUC = {test_auc:.3f})')\n    ax_roc.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.500)')\n    ax_roc.set_xlabel('False Positive Rate')\n    ax_roc.set_ylabel('True Positive Rate')\n    ax_roc.set_title('ROC Curve')\n    ax_roc.legend(loc='lower right')\n    ax_roc.grid(True, alpha=0.3)\n    fig_roc.tight_layout()\n    fig_roc.savefig('roc_curve.png', dpi=150)\n    mlflow.log_artifact('roc_curve.png')\n    plt.show()\n\n    # Feature Importance\n    importance_df = pd.DataFrame({\n        'feature': feature_cols,\n        'importance': model.feature_importance(importance_type='gain')\n    }).sort_values('importance', ascending=False)\n\n    fig_imp, ax_imp = plt.subplots(figsize=(10, 8))\n    top_20 = importance_df.head(20)\n    ax_imp.barh(range(len(top_20)), top_20['importance'].values)\n    ax_imp.set_yticks(range(len(top_20)))\n    ax_imp.set_yticklabels(top_20['feature'].values)\n    ax_imp.set_xlabel('Importance (Gain)')\n    ax_imp.set_title('Top 20 Feature Importances')\n    ax_imp.invert_yaxis()\n    fig_imp.tight_layout()\n    fig_imp.savefig('feature_importance.png', dpi=150)\n    mlflow.log_artifact('feature_importance.png')\n    plt.show()\n\n    importance_df.to_csv('feature_importance.csv', index=False)\n    mlflow.log_artifact('feature_importance.csv')\n\n    mlflow.lightgbm.log_model(\n        model,\n        artifact_path=\"model\",\n        registered_model_name=\"offer_recommender\"\n    )\n\n    import json\n    with open('feature_cols.json', 'w') as f:\n        json.dump(feature_cols, f)\n    mlflow.log_artifact('feature_cols.json')\n\n    run_id = mlflow.active_run().info.run_id\n\nauc = test_auc\nprint(f\"Test AUC: {test_auc:.4f} | Train AUC: {train_auc:.4f} | Gap: {train_auc - test_auc:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "mlflow_ui",
   "metadata": {},
   "source": [
    "### 10. View MLflow Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow_summary",
   "metadata": {},
   "outputs": [],
   "source": "experiment = mlflow.get_experiment_by_name(\"Offer_Recommendation_Ranking\")\nruns_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n\ndisplay_cols = ['run_id', 'metrics.test_auc', 'metrics.train_auc', 'metrics.auc_gap',\n                'metrics.precision_at_0.5', 'metrics.recall_at_0.5']\navailable_cols = [c for c in display_cols if c in runs_df.columns]\nruns_df[available_cols]"
  },
  {
   "cell_type": "markdown",
   "id": "save_section",
   "metadata": {},
   "source": [
    "### 11. Save Model & Artifacts (Local Copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport pickle\n\nos.makedirs('../models', exist_ok=True)\n\nmodel.save_model('../models/offer_recommender_v2.txt')\n\nwith open('../models/feature_cols_v2.json', 'w') as f:\n    json.dump(feature_cols, f)\n\nwith open('../models/kmeans_model.pkl', 'wb') as f:\n    pickle.dump(kmeans, f)\n\nwith open('../models/scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\nwith open('../models/cluster_candidates.json', 'w') as f:\n    json.dump({str(k): v for k, v in cluster_candidates.items()}, f)\n\nprint(\"Saved: model, features, kmeans, scaler, cluster_candidates\")"
  },
  {
   "cell_type": "markdown",
   "id": "redis_section",
   "metadata": {},
   "source": [
    "### 12. Load Customer Features + Clusters into Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "redis_connect",
   "metadata": {},
   "outputs": [],
   "source": "import redis\n\ncache = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\ncache.ping()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "redis_upload",
   "metadata": {},
   "outputs": [],
   "source": "pipe = cache.pipeline()\nbatch_size = 10000\ncount = 0\n\nfor _, row in df_features.iterrows():\n    customer_id = row['customer_id']\n    features = row.drop('customer_id').to_dict()\n    features = {k: float(v) if isinstance(v, (np.integer, np.floating)) else v\n                for k, v in features.items()}\n    pipe.set(f\"cust:{customer_id}\", json.dumps(features))\n    count += 1\n    if count % batch_size == 0:\n        pipe.execute()\n\npipe.execute()\nprint(f\"Loaded {count:,} customers to Redis\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "redis_verify",
   "metadata": {},
   "outputs": [],
   "source": "sample_id = df_features['customer_id'].iloc[0]\nstored = json.loads(cache.get(f\"cust:{sample_id}\"))\n\nprint(f\"Sample: cluster={stored.get('cluster')}, frequency={stored.get('frequency')}, recency={stored.get('recency_days')}\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Stage 1 (KMeans): {N_CLUSTERS} clusters, {RETRIEVAL_TOP_N} candidates each\")\nprint(f\"Stage 2 (LightGBM): AUC={auc:.4f}, {len(feature_cols)} features ({len(interaction_cols)} interactions)\")\nprint(f\"Redis: {count:,} customers loaded\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}